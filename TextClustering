library(tm)
#Read movie hashtags into a data frame
setwd("C:/Users/gyana/OneDrive/Documents/R/WB/AIPM/project/CSV")
pixstory <- read.csv("1000.csv")
pixstory

#Load hastags into a corpus
narration <- VCorpus(VectorSource(pixstory$Narration))

#replace comma with spaces
clean_narration <- tm_map(narration, 
                         content_transformer(
                            function(x) gsub(","," ",x)
                            )
                         )

inspect(clean_narration[[1]])

clean_narration <- tm_map(clean_narration,removeWords,stopwords())
analyze_corpus("Removed Stopwords",clean_narration)

clean_narration <- tm_map(clean_narration, removePunctuation)
analyze_corpus("Removed punctuations",clean_narration)

clean_narration <- tm_map(clean_narration, content_transformer(tolower))
analyze_corpus("Converted to lower case",clean_narration)


#Generate the Document Term matrix
narration_dtm <- DocumentTermMatrix(clean_narration)

#Inspect to Document Term matrix
inspect(narration_dtm)



#                  K-Means Clustering

#Setting the seed ensures repeatable results
set.seed(100)

#Create 3 clusters
narration_clusters <-  kmeans(narration_dtm, 5)

#Inspect the results
narration_clusters$cluster
narration_clusters
saveRDS(narration_clusters$cluster,"narration_cluster.rds")

#Add cluster information to the original data frame 
for ( story in 1:nrow(pixstory)) {
  pixstory$Cluster[story] = narration_clusters$cluster[story]
}

#Sort by cluster and review results
print(pixstory[order(pixstory$Cluster),c(1,3)]  )
saveRDS(pixstory$Cluster,"pixstory_cluster.rds")
write.csv(pixstory$Cluster,"pixstory_cluster.csv")

#                  K-Means Optimization

#Function to find the optimum no. of clusters
optimal_cluster_plot <- function(data, iterations=10, seed=1000){
  
  #Set within-sum-of-squares for a single cluster
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  
  #Iterate upto 10 clusters and measure wss.
  for (i in 2:iterations){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)
  }
  
  #Plot wss for each value of k and find the elbow
  plot(1:iterations, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares", col="red")}

#Execute the function
optimal_cluster_plot(narration_dtm)
saveRDS(optimal_cluster_plot(narration_dtm),"optimal_cluster.rds")

#--------------------------------------------------------------------------

# Creating a hierarchical cluster dendrogram
tdm <- narration_dtm
d<-dist(tdm)
saveRDS(d, "dist_tdm.rds")
#rownames(d) <- tdm
hc<-hclust(d)
plot(hc)
#library("rafalib")
#myplclust(hc, labels=hc$labels)
rect.hclust(hc,k=5)#k can be any value that you'd like to skin the cat :)

# Bag of words----

# Load qdap
library(qdap)

# Print new_text to the console
new_text <- clean_narration
new_text

# Find the 10 most frequent terms: term_count
term_count <- freq_terms(new_text, 10)

# Plot term_count
plot(term_count)

